# ValCortex + vicuna-13b Setup (Windows)

## 0. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ quant Q4_K_M (‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û/‡πÅ‡∏£‡∏°)

# ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
```powershell
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Q4_K_M ~7.9GB)
```powershell
curl.exe -L "https://huggingface.co/cjpais/llava-v1.6-vicuna-13b-gguf/resolve/main/llava-v1.6-vicuna-13b.Q4_K_M.gguf?download=true" -o "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf"
```


# ValCortex + GPT-OSS-20B Setup (Windows)

## 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

> ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `G:\models`

```powershell
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

```powershell
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå quant ‡πÅ‡∏ö‡∏ö Q4_K_M (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏£‡∏° ~16GB+)
curl.exe -L "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf?download=true" -o "G:\models\gpt-oss-20b-Q4_K_M.gguf"
```

üìå ‡∏ó‡∏µ‡πà‡∏°‡∏≤ Hugging Face: [unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  
(‡πÄ‡∏•‡∏∑‡∏≠‡∏Å quant ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô Q5_K_M, Q8_0 ‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)

---

## 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Binary

> ‡πÑ‡∏ö‡∏ô‡∏≤‡∏£‡∏µ/EXE ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà `G:\ai`

```powershell
New-Item -ItemType Directory -Force -Path G:\ai | Out-Null
cd G:\ai
git clone https://github.com/geekp2p/ValCortex
```

---

## 3. Build API Binary (valcortex-api.exe)
wget "https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe" -OutFile "G:\ai\Miniconda3-latest-Windows-x86_64.exe"

```powershell
cd G:\ai\ValCortex\cortex
# conda create -n valcortex python=3.10 -y
conda create -n valcortex -c conda-forge python=3.10 -y
conda activate valcortex
pip install -r requirements.txt pyinstaller
pyinstaller --onefile --name valcortex-api start_api.py
```

set OLLAMA_BASE_URL=http://127.0.0.1:11434
set OLLAMA_TEXT_MODEL=gpt-oss:20b
set OLLAMA_VISION_MODEL=llava:13b



### ‡∏£‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```powershell
cd dist
.\valcortex-api.exe
```

---

## 4. Build Local Model Binary (‡∏ù‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)

```powershell
cd G:\ai\ValCortex\local_model
conda create -n valmodel python=3.10 -y
conda activate valmodel
pip install -r requirements.txt pyinstaller
pyinstaller --onefile --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;gpt-oss-20b-Q4_K_M.gguf" --name local-model app.py
```

### ‡∏£‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```powershell
cd dist
.\local-model.exe
```

---

## 5. Build Standalone Binary (API + Ollama)

```powershell
cd G:\ai\ValCortex\cortex
conda activate valcortex
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î ollama.exe ‡∏°‡∏≤‡∏ß‡∏≤‡∏á‡πÑ‡∏ß‡πâ‡πÉ‡∏ô‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ô (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
# curl.exe -L "https://github.com/jmorganca/ollama/releases/latest/download/ollama-windows-amd64.exe" -o ollama.exe && pyinstaller --onefile --add-binary "ollama.exe;." --name valcortex-all standalone.py

curl.exe -L "https://github.com/ollama/ollama/releases/latest/download/ollama-windows-amd64.zip" -o ollama.zip
then unzip


cd G:\ai\ValCortex\cortex
# curl.exe -L "https://github.com/jmorganca/ollama/releases/latest/download/ollama-windows-amd64.exe" -o ollama.exe
pyinstaller --onefile --add-binary "ollama.exe;." --name valcortex-all standalone.py

curl.exe -L "https://github.com/jmorganca/ollama/releases/latest/download/ollama-windows-amd64.exe" -o cortex\ollama.exe
pyinstaller --onefile --add-binary "cortex\ollama.exe;." --name valcortex-all cortex\standalone.py



```

### ‡∏£‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```powershell
cd dist
.\valcortex-all.exe
```

---

## 6. Notes

- `valcortex-api.exe` ‚Üí ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ HTTP API (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å)
- `local-model.exe` ‚Üí ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• `.gguf` ‡∏ó‡∏µ‡πà‡∏ù‡∏±‡∏á‡πÑ‡∏ß‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô binary
- `valcortex-all.exe` ‚Üí ‡πÄ‡∏õ‡∏¥‡∏î‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ Ollama ‡πÅ‡∏•‡∏∞ API ‡πÉ‡∏ô‡πÇ‡∏õ‡∏£‡πÅ‡∏Å‡∏£‡∏°‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
- ‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ Ollama ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama service ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `OLLAMA_MODELS=G:\models`
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô [LM Studio](https://lmstudio.ai) ‡∏´‡∏£‡∏∑‡∏≠ [llama.cpp](https://github.com/ggerganov/llama.cpp) ‡πÑ‡∏î‡πâ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô

---

‚úÖ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•, build binary ‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (`G:\models` / `G:\ai`) ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß



pyinstaller --onefile --name local-model-text --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" app_text.py
pyinstaller --onefile --name local-model-vision app_vision.py
pyinstaller --onefile --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" embed_cli.py


pyinstaller --onefile --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" .\embed_cli.py



pyinstaller --onedir --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" embed_cli.py


pip install llama-cpp-python

pyinstaller --onedir --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" --hidden-import=llama_cpp embed_cli.py

.\dist\embed-cli\embed-cli.exe --prompt "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡πÇ‡∏•‡∏Å"

python -m pip install llama-cpp-python --index-url https://abetlen.github.io/llama-cpp-python/whl/cpu --extra-index-url https://pypi.org/simple --prefer-binary

python -c "import llama_cpp; print('llama_cpp version:', llama_cpp.__version__)"

python -m pip install pyinstaller



taskkill /f /im embed-cli.exe 2>nul

rmdir /s /q dist\embed-cli

pyinstaller --onedir --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;
models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" --hidden-import=llama_cpp embed_cli.py --noconfirm



‚úÖ pyinstaller --onedir --name embed-cli --collect-data llama_cpp --collect-binaries llama_cpp --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" embed_cli.py --noconfirm
‚úÖ .\dist\embed-cli\embed-cli.exe --prompt "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡πÇ‡∏•‡∏Å"

Testing
‚úÖ .\dist\embed-cli\embed-cli.exe --prompt "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡πÇ‡∏•‡∏Å"


pyinstaller --onedir --name embed-cli --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" --add-data "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf;models\llava-v1.6-vicuna-13b.Q4_K_M.gguf" embed_cli.py

.\dist\embed-cli\embed-cli.exe --prompt "‡∏™‡∏ß‡∏±‡∏™‡∏î‡∏µ ‡πÇ‡∏•‡∏Å"


pyinstaller --onefile --name local-model-text --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;models\gpt-oss-20b-Q4_K_M.gguf" app_text.py
pyinstaller --onefile --name local-model-vision app_vision.py
