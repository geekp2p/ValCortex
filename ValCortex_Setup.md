# ValCortex + vicuna-13b Setup (Windows)

## 0. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏î‡πâ‡∏ß‡∏¢ quant Q4_K_M (‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏Ñ‡∏∏‡∏ì‡∏†‡∏≤‡∏û/‡πÅ‡∏£‡∏°)

# ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏õ‡∏•‡∏≤‡∏¢‡∏ó‡∏≤‡∏á
```powershell
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏î‡∏µ‡∏¢‡∏ß (Q4_K_M ~7.9GB)
```powershell
curl.exe -L "https://huggingface.co/cjpais/llava-v1.6-vicuna-13b-gguf/resolve/main/llava-v1.6-vicuna-13b.Q4_K_M.gguf?download=true" -o "G:\models\llava-v1.6-vicuna-13b.Q4_K_M.gguf"
```


# ValCortex + GPT-OSS-20B Setup (Windows)

## 1. ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• (.gguf)

> ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÄ‡∏Å‡πá‡∏ö‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà `G:\models`

```powershell
# ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• (‡∏ñ‡πâ‡∏≤‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡∏°‡∏µ)
New-Item -ItemType Directory -Force -Path G:\models | Out-Null
```

```powershell
# ‡∏î‡∏≤‡∏ß‡∏ô‡πå‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå quant ‡πÅ‡∏ö‡∏ö Q4_K_M (‡∏õ‡∏£‡∏∞‡∏´‡∏¢‡∏±‡∏î‡πÅ‡∏£‡∏° ~16GB+)
curl.exe -L "https://huggingface.co/unsloth/gpt-oss-20b-GGUF/resolve/main/gpt-oss-20b-Q4_K_M.gguf?download=true" -o "G:\models\gpt-oss-20b-Q4_K_M.gguf"
```

üìå ‡∏ó‡∏µ‡πà‡∏°‡∏≤ Hugging Face: [unsloth/gpt-oss-20b-GGUF](https://huggingface.co/unsloth/gpt-oss-20b-GGUF)  
(‡πÄ‡∏•‡∏∑‡∏≠‡∏Å quant ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÑ‡∏î‡πâ ‡πÄ‡∏ä‡πà‡∏ô Q5_K_M, Q8_0 ‡∏ï‡∏≤‡∏°‡∏™‡πÄ‡∏õ‡∏Å‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á)

---

## 2. ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏°‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Binary

> ‡πÑ‡∏ö‡∏ô‡∏≤‡∏£‡∏µ/EXE ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì‡∏à‡∏∞‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà `G:\ai`

```powershell
New-Item -ItemType Directory -Force -Path G:\ai | Out-Null
cd G:\ai
git clone https://github.com/geekp2p/ValCortex
```

---

## 3. Build API Binary (valcortex-api.exe)
wget "https://repo.anaconda.com/miniconda/Miniconda3-latest-Windows-x86_64.exe" -OutFile "G:\ai\Miniconda3-latest-Windows-x86_64.exe"

```powershell
cd G:\ai\ValCortex\cortex
# conda create -n valcortex python=3.10 -y
conda create -n valcortex -c conda-forge python=3.10 -y
conda activate valcortex
pip install -r requirements.txt pyinstaller
pyinstaller --onefile --name valcortex-api start_api.py
```

set OLLAMA_BASE_URL=http://127.0.0.1:11434
set OLLAMA_TEXT_MODEL=gpt-oss:20b
set OLLAMA_VISION_MODEL=llava:13b



### ‡∏£‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```powershell
cd dist
.\valcortex-api.exe
```

---

## 4. Build Local Model Binary (‡∏ù‡∏±‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•)

```powershell
cd G:\ai\ValCortex\local_model
conda create -n valmodel python=3.10 -y
conda activate valmodel
pip install -r requirements.txt pyinstaller
pyinstaller --onefile --add-data "G:\models\gpt-oss-20b-Q4_K_M.gguf;gpt-oss-20b-Q4_K_M.gguf" --name local-model app.py
```

### ‡∏£‡∏±‡∏ô‡∏ó‡∏î‡∏™‡∏≠‡∏ö
```powershell
cd dist
.\local-model.exe
```

---

## 5. Notes

- `valcortex-api.exe` ‚Üí ‡πÉ‡∏´‡πâ‡∏ö‡∏£‡∏¥‡∏Å‡∏≤‡∏£ HTTP API (‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡∏ï‡πà‡∏≠‡∏†‡∏≤‡∏¢‡∏ô‡∏≠‡∏Å)  
- `local-model.exe` ‚Üí ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏• `.gguf` ‡∏ó‡∏µ‡πà‡∏ù‡∏±‡∏á‡πÑ‡∏ß‡πâ‡∏†‡∏≤‡∏¢‡πÉ‡∏ô binary  
- ‡∏´‡∏≤‡∏Å‡πÉ‡∏ä‡πâ Ollama ‚Üí ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ Ollama service ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏≠‡∏¢‡∏π‡πà ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤ `OLLAMA_MODELS=G:\models`  
- ‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥‡πÉ‡∏´‡πâ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏ô [LM Studio](https://lmstudio.ai) ‡∏´‡∏£‡∏∑‡∏≠ [llama.cpp](https://github.com/ggerganov/llama.cpp) ‡πÑ‡∏î‡πâ‡πÄ‡∏ä‡πà‡∏ô‡∏Å‡∏±‡∏ô  

---

‚úÖ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏Ñ‡∏∏‡∏ì‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•, build binary ‡πÅ‡∏•‡∏∞‡∏ß‡∏≤‡∏á‡πÉ‡∏ô‡∏ï‡∏≥‡πÅ‡∏´‡∏ô‡πà‡∏á‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ (`G:\models` / `G:\ai`) ‡πÑ‡∏î‡πâ‡∏Ñ‡∏£‡∏ö‡πÅ‡∏•‡πâ‡∏ß
